{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit6102001/Python-Machine-learning-Deep-learning/blob/main/how_to_save_amodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e664d105-2cca-45e0-8ed2-73d354518dc0",
      "metadata": {
        "id": "e664d105-2cca-45e0-8ed2-73d354518dc0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Conv2D,MaxPooling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6cc208-2e37-47e6-a3d8-6633f5a2816c",
      "metadata": {
        "id": "6a6cc208-2e37-47e6-a3d8-6633f5a2816c",
        "outputId": "c787a887-2b70-4e5c-92e3-2af2fd55cc19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 300 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_gen=ImageDataGenerator(rescale=1/255.0,zoom_range=2,vertical_flip=True,horizontal_flip=True,rotation_range=30)\n",
        "train_itr=train_gen.flow_from_directory(\"d:/dataset/gender_training/train/\",color_mode='grayscale',target_size=(90,90),batch_size=50,class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c233c3e2-8639-4f46-bfa6-87110392668d",
      "metadata": {
        "id": "c233c3e2-8639-4f46-bfa6-87110392668d",
        "outputId": "06a62204-77e8-4af5-c0e6-d0c0a6e2715d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 38 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "valid_gen=ImageDataGenerator(rescale=1/255.0,zoom_range=2,vertical_flip=True,horizontal_flip=True,rotation_range=30)\n",
        "valid_itr=train_gen.flow_from_directory(\"d:/dataset/gender_training/val/\",color_mode='grayscale',target_size=(90,90),batch_size=50,class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81989b16-24b2-4be9-b819-c820b40b3b66",
      "metadata": {
        "id": "81989b16-24b2-4be9-b819-c820b40b3b66",
        "outputId": "f0b645a6-bc70-4fd0-8b03-1e4ad44be17b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "6/6 [==============================] - 6s 867ms/step - loss: 0.7631 - accuracy: 0.5167 - val_loss: 0.6896 - val_accuracy: 0.5526\n",
            "Epoch 2/40\n",
            "6/6 [==============================] - 3s 417ms/step - loss: 0.6887 - accuracy: 0.5500 - val_loss: 0.6878 - val_accuracy: 0.5526\n",
            "Epoch 3/40\n",
            "6/6 [==============================] - 3s 492ms/step - loss: 0.6774 - accuracy: 0.6267 - val_loss: 0.6691 - val_accuracy: 0.6842\n",
            "Epoch 4/40\n",
            "6/6 [==============================] - 3s 485ms/step - loss: 0.6463 - accuracy: 0.6900 - val_loss: 0.6044 - val_accuracy: 0.7105\n",
            "Epoch 5/40\n",
            "6/6 [==============================] - 3s 491ms/step - loss: 0.6190 - accuracy: 0.6300 - val_loss: 0.6260 - val_accuracy: 0.7105\n",
            "Epoch 6/40\n",
            "6/6 [==============================] - 3s 466ms/step - loss: 0.6261 - accuracy: 0.6000 - val_loss: 0.7301 - val_accuracy: 0.6316\n",
            "Epoch 7/40\n",
            "6/6 [==============================] - 3s 458ms/step - loss: 0.6259 - accuracy: 0.6167 - val_loss: 0.5647 - val_accuracy: 0.6579\n",
            "Epoch 8/40\n",
            "6/6 [==============================] - 3s 469ms/step - loss: 0.5921 - accuracy: 0.6567 - val_loss: 0.6602 - val_accuracy: 0.5263\n",
            "Epoch 9/40\n",
            "6/6 [==============================] - 3s 440ms/step - loss: 0.5546 - accuracy: 0.7267 - val_loss: 0.6961 - val_accuracy: 0.4474\n",
            "Epoch 10/40\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 0.5233 - accuracy: 0.6867 - val_loss: 0.6672 - val_accuracy: 0.6053\n",
            "Epoch 11/40\n",
            "6/6 [==============================] - 3s 484ms/step - loss: 0.5530 - accuracy: 0.7100 - val_loss: 0.6996 - val_accuracy: 0.6053\n",
            "Epoch 12/40\n",
            "6/6 [==============================] - 3s 445ms/step - loss: 0.5037 - accuracy: 0.7200 - val_loss: 0.6009 - val_accuracy: 0.6579\n",
            "Epoch 13/40\n",
            "6/6 [==============================] - 3s 425ms/step - loss: 0.5434 - accuracy: 0.6967 - val_loss: 0.6016 - val_accuracy: 0.7105\n",
            "Epoch 14/40\n",
            "6/6 [==============================] - 2s 403ms/step - loss: 0.5371 - accuracy: 0.7100 - val_loss: 0.7343 - val_accuracy: 0.4474\n",
            "Epoch 15/40\n",
            "6/6 [==============================] - 2s 412ms/step - loss: 0.5351 - accuracy: 0.7167 - val_loss: 0.6223 - val_accuracy: 0.5526\n",
            "Epoch 16/40\n",
            "6/6 [==============================] - 3s 451ms/step - loss: 0.5065 - accuracy: 0.7633 - val_loss: 0.5705 - val_accuracy: 0.6316\n",
            "Epoch 17/40\n",
            "6/6 [==============================] - 3s 418ms/step - loss: 0.5364 - accuracy: 0.7400 - val_loss: 0.5476 - val_accuracy: 0.7105\n",
            "Epoch 18/40\n",
            "6/6 [==============================] - 2s 410ms/step - loss: 0.4999 - accuracy: 0.7300 - val_loss: 0.6324 - val_accuracy: 0.6316\n",
            "Epoch 19/40\n",
            "6/6 [==============================] - 3s 433ms/step - loss: 0.5421 - accuracy: 0.7333 - val_loss: 0.6099 - val_accuracy: 0.6316\n",
            "Epoch 20/40\n",
            "6/6 [==============================] - 3s 452ms/step - loss: 0.4920 - accuracy: 0.7167 - val_loss: 0.4473 - val_accuracy: 0.7895\n",
            "Epoch 21/40\n",
            "6/6 [==============================] - 3s 408ms/step - loss: 0.4684 - accuracy: 0.7433 - val_loss: 0.5190 - val_accuracy: 0.7105\n",
            "Epoch 22/40\n",
            "6/6 [==============================] - 2s 400ms/step - loss: 0.5171 - accuracy: 0.7200 - val_loss: 0.6054 - val_accuracy: 0.6316\n",
            "Epoch 23/40\n",
            "6/6 [==============================] - 2s 396ms/step - loss: 0.5274 - accuracy: 0.7033 - val_loss: 0.6511 - val_accuracy: 0.6053\n",
            "Epoch 24/40\n",
            "6/6 [==============================] - 2s 395ms/step - loss: 0.5284 - accuracy: 0.7467 - val_loss: 0.6590 - val_accuracy: 0.6579\n",
            "Epoch 25/40\n",
            "6/6 [==============================] - 3s 438ms/step - loss: 0.4610 - accuracy: 0.7767 - val_loss: 0.7378 - val_accuracy: 0.6316\n",
            "Epoch 26/40\n",
            "6/6 [==============================] - 3s 441ms/step - loss: 0.4683 - accuracy: 0.7633 - val_loss: 0.5659 - val_accuracy: 0.6316\n",
            "Epoch 27/40\n",
            "6/6 [==============================] - 2s 393ms/step - loss: 0.5272 - accuracy: 0.6867 - val_loss: 0.7219 - val_accuracy: 0.6053\n",
            "Epoch 28/40\n",
            "6/6 [==============================] - 2s 400ms/step - loss: 0.5083 - accuracy: 0.7267 - val_loss: 0.5803 - val_accuracy: 0.6053\n",
            "Epoch 29/40\n",
            "6/6 [==============================] - 2s 404ms/step - loss: 0.5181 - accuracy: 0.7200 - val_loss: 0.4402 - val_accuracy: 0.7368\n",
            "Epoch 30/40\n",
            "6/6 [==============================] - 3s 487ms/step - loss: 0.4799 - accuracy: 0.7433 - val_loss: 0.6305 - val_accuracy: 0.6053\n",
            "Epoch 31/40\n",
            "6/6 [==============================] - 3s 443ms/step - loss: 0.4600 - accuracy: 0.7667 - val_loss: 0.6666 - val_accuracy: 0.7105\n",
            "Epoch 32/40\n",
            "6/6 [==============================] - 2s 403ms/step - loss: 0.4703 - accuracy: 0.7667 - val_loss: 0.5224 - val_accuracy: 0.7895\n",
            "Epoch 33/40\n",
            "6/6 [==============================] - 2s 413ms/step - loss: 0.4855 - accuracy: 0.7500 - val_loss: 0.7266 - val_accuracy: 0.6053\n",
            "Epoch 34/40\n",
            "6/6 [==============================] - 2s 393ms/step - loss: 0.4906 - accuracy: 0.7500 - val_loss: 0.6000 - val_accuracy: 0.5789\n",
            "Epoch 35/40\n",
            "6/6 [==============================] - 3s 443ms/step - loss: 0.4792 - accuracy: 0.7600 - val_loss: 0.5914 - val_accuracy: 0.6842\n",
            "Epoch 36/40\n",
            "6/6 [==============================] - 3s 413ms/step - loss: 0.4833 - accuracy: 0.7600 - val_loss: 0.6387 - val_accuracy: 0.6579\n",
            "Epoch 37/40\n",
            "6/6 [==============================] - 2s 392ms/step - loss: 0.4937 - accuracy: 0.7667 - val_loss: 0.6313 - val_accuracy: 0.6579\n",
            "Epoch 38/40\n",
            "6/6 [==============================] - 2s 378ms/step - loss: 0.5123 - accuracy: 0.7433 - val_loss: 0.5628 - val_accuracy: 0.7105\n",
            "Epoch 39/40\n",
            "6/6 [==============================] - 2s 375ms/step - loss: 0.4326 - accuracy: 0.7867 - val_loss: 0.6104 - val_accuracy: 0.7105\n",
            "Epoch 40/40\n",
            "6/6 [==============================] - 2s 382ms/step - loss: 0.5343 - accuracy: 0.7133 - val_loss: 0.6576 - val_accuracy: 0.6053\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x19219446be0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D(64,(3,3),activation='relu',input_shape=(90,90,1)))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200,activation='relu'))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(train_itr,epochs=40,steps_per_epoch=300//50,validation_data=valid_itr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa0448b-7b76-4c2a-8e49-678b4873ecda",
      "metadata": {
        "id": "baa0448b-7b76-4c2a-8e49-678b4873ecda",
        "outputId": "b8af85e6-dd69-4c62-aae0-abaa84b74047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 60 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_gen=ImageDataGenerator(rescale=1/255.0,zoom_range=2,vertical_flip=True,horizontal_flip=True,rotation_range=30)\n",
        "test_itr=train_gen.flow_from_directory(\"d:/dataset/gender_training/test/\",color_mode='grayscale',target_size=(90,90),batch_size=50,class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2c4e4c-2270-4431-8c2c-f3ed86d8c433",
      "metadata": {
        "id": "0c2c4e4c-2270-4431-8c2c-f3ed86d8c433",
        "outputId": "10ad623b-3009-4977-f30d-365604dcb715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 70ms/step - loss: 0.5366 - accuracy: 0.7000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.5365661978721619, 0.699999988079071]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_itr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8261761-79a1-4617-a90b-d19b371f7255",
      "metadata": {
        "id": "e8261761-79a1-4617-a90b-d19b371f7255",
        "outputId": "c003eaa2-7ac2-46d0-cc04-73386332b9db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 88, 88, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 44, 44, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 42, 42, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 21, 21, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 28224)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 200)               5645000   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 202       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,702,870\n",
            "Trainable params: 5,702,870\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366b494d-a882-4419-8d0b-a23b8da91512",
      "metadata": {
        "id": "366b494d-a882-4419-8d0b-a23b8da91512"
      },
      "outputs": [],
      "source": [
        "model.save(\"d:/gender_model.h5\") #save both configuration and weights into single file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7db874-a1c6-4e50-a560-391786ddfc36",
      "metadata": {
        "id": "5d7db874-a1c6-4e50-a560-391786ddfc36"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a81721-d57d-4b4e-9e4e-babcd61aac76",
      "metadata": {
        "id": "53a81721-d57d-4b4e-9e4e-babcd61aac76",
        "outputId": "6336b560-7f91-4138-f044-3fb2c53d61c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "img=load_img(\"d:/images/cricketers/sachin.jpg\",color_mode='grayscale',target_size=(90,90))\n",
        "np_img=img_to_array(img)\n",
        "#print(np_img.shape)\n",
        "#print(np_img.reshape(1,90,90,1).shape)\n",
        "print(model.predict(np_img.reshape(1,90,90,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01576ec8-97a9-4268-b724-b49818a4ab6b",
      "metadata": {
        "id": "01576ec8-97a9-4268-b724-b49818a4ab6b",
        "outputId": "35a9a056-2e99-455e-b661-cd4c79081868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "img=load_img(\"d:/images/players/lata.jpg\",color_mode='grayscale',target_size=(90,90))\n",
        "np_img=img_to_array(img)\n",
        "#print(np_img.shape)\n",
        "#print(np_img.reshape(1,90,90,1).shape)\n",
        "print(model.predict(np_img.reshape(1,90,90,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3bdf602-f4f8-48b6-8c13-8d2ab594018a",
      "metadata": {
        "id": "d3bdf602-f4f8-48b6-8c13-8d2ab594018a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "how_to_save_amodel.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}