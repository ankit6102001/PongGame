{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit6102001/Python-Machine-learning-Deep-learning/blob/main/face_eye_detection_in_video_%26_Gender_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21fd19bd-874e-48d0-95b4-175a53e22349",
      "metadata": {
        "id": "21fd19bd-874e-48d0-95b4-175a53e22349"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd282065-52f7-4066-8f66-32100dba5f9e",
      "metadata": {
        "id": "dd282065-52f7-4066-8f66-32100dba5f9e"
      },
      "outputs": [],
      "source": [
        "faceModel=cv2.CascadeClassifier(\"d:/dataset/haar/haarcascade_frontalface_default.xml\")\n",
        "eyesModel=cv2.CascadeClassifier(\"d:/dataset/haar/haarcascade_eye.xml\")\n",
        "img=cv2.imread(\"d:/images/cricketers/dhoni.jpg\")\n",
        "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "faces=faceModel.detectMultiScale(gray,maxSize=(200,150),minNeighbors=5,scaleFactor=1.2)\n",
        "\n",
        "for x,y,w,h in faces:\n",
        "    cv2.rectangle(img,(x,y,w,h),(0,0,255),2)\n",
        "    face=gray[y:y+h,x:x+w]\n",
        "    eyes=eyesModel.detectMultiScale(face,maxSize=(50,30))\n",
        "\n",
        "    for ex,ey,ew,eh in eyes:\n",
        "        cv2.rectangle(img,(x+ex,y+ey,ew,eh),(255,0,0),2)\n",
        "\n",
        "cv2.imshow(\"iamge\",img)\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabcb3b0-ca8a-436d-88fc-de93bd0a5231",
      "metadata": {
        "id": "eabcb3b0-ca8a-436d-88fc-de93bd0a5231"
      },
      "outputs": [],
      "source": [
        "vdo=cv2.VideoCapture(\"d:/Python Technology Overview By Aditya Sir.mp4\")\n",
        "flag,img=vdo.read()\n",
        "cv2.imshow(\"image\",img)\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "flag,img=vdo.read()\n",
        "cv2.imshow(\"image\",img)\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c79b39fd-cbe8-4026-812a-dabe448ac0e1",
      "metadata": {
        "id": "c79b39fd-cbe8-4026-812a-dabe448ac0e1"
      },
      "outputs": [],
      "source": [
        "vdo=cv2.VideoCapture(\"d:/Python Technology Overview By Aditya Sir.mp4\")\n",
        "faceModel=cv2.CascadeClassifier(\"d:/dataset/haar/haarcascade_frontalface_default.xml\")\n",
        "eyesModel=cv2.CascadeClassifier(\"d:/dataset/haar/haarcascade_eye.xml\")\n",
        "while(True):\n",
        "    flag,img=vdo.read()\n",
        "    if(flag==False):\n",
        "        break\n",
        "    cv2.putText(img,\"Press 'c' to cancel..\",(10,40),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
        "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    faces=faceModel.detectMultiScale(gray,maxSize=(200,150),minNeighbors=5,scaleFactor=1.2)\n",
        "\n",
        "    for x,y,w,h in faces:\n",
        "        cv2.rectangle(img,(x,y,w,h),(0,0,255),2)\n",
        "        \n",
        "        face=gray[y:y+h,x:x+w]\n",
        "        eyes=eyesModel.detectMultiScale(face,maxSize=(50,30))\n",
        "\n",
        "        for ex,ey,ew,eh in eyes:\n",
        "            cv2.rectangle(img,(x+ex,y+ey,ew,eh),(255,0,0),2)\n",
        "    \n",
        "    cv2.imshow(\"image\",img)\n",
        "    key=cv2.waitKey(50)\n",
        "    if(key==ord('c')):\n",
        "        break\n",
        "cv2.destroyAllWindows()\n",
        "vdo.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ddab36-55c3-48e1-bb8c-175b43c3387f",
      "metadata": {
        "id": "14ddab36-55c3-48e1-bb8c-175b43c3387f"
      },
      "outputs": [],
      "source": [
        "vdo=cv2.VideoCapture(0)\n",
        "while(True):\n",
        "    flag,img=vdo.read()\n",
        "    if(flag==False):\n",
        "        break\n",
        "    cv2.putText(img,\"Press 'c' to cancel..\",(10,40),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
        "    cv2.imshow(\"image\",img)\n",
        "    key=cv2.waitKey(50)\n",
        "    if(key==ord('c')):\n",
        "        break\n",
        "cv2.destroyAllWindows()\n",
        "vdo.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0771dbc-e7a0-4aaa-a21a-6574a546fd09",
      "metadata": {
        "id": "b0771dbc-e7a0-4aaa-a21a-6574a546fd09"
      },
      "outputs": [],
      "source": [
        "vdo=cv2.VideoCapture(0)\n",
        "faceModel=cv2.CascadeClassifier(\"d:/dataset/haar/haarcascade_frontalface_default.xml\")\n",
        "eyesModel=cv2.CascadeClassifier(\"d:/dataset/haar/haarcascade_eye.xml\")\n",
        "while(True):\n",
        "    flag,img=vdo.read()\n",
        "    if(flag==False):\n",
        "        break\n",
        "    cv2.putText(img,\"Press 'c' to cancel..\",(10,40),cv2.FONT_HERSHEY_PLAIN,2,(255,0,0),2)\n",
        "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    faces=faceModel.detectMultiScale(gray,minNeighbors=5,scaleFactor=1.2)\n",
        "\n",
        "    for x,y,w,h in faces:\n",
        "        cv2.rectangle(img,(x,y,w,h),(0,0,255),2)\n",
        "        \n",
        "        face=gray[y:y+h,x:x+w]\n",
        "        eyes=eyesModel.detectMultiScale(face,maxSize=(50,30))\n",
        "\n",
        "        for ex,ey,ew,eh in eyes:\n",
        "            cv2.rectangle(img,(x+ex,y+ey,ew,eh),(255,0,0),2)\n",
        "    \n",
        "    cv2.imshow(\"image\",img)\n",
        "    key=cv2.waitKey(50)\n",
        "    if(key==ord('c')):\n",
        "        break\n",
        "cv2.destroyAllWindows()\n",
        "vdo.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2a3620-eb3a-45ae-98fc-a58806e3cb03",
      "metadata": {
        "id": "3f2a3620-eb3a-45ae-98fc-a58806e3cb03",
        "outputId": "ecde2b65-b098-47c2-df8f-758e466b2e88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([138, 112,  91, ...,  29,  26,  27], dtype=uint8)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X=[]\n",
        "\n",
        "img=cv2.imread(\"gender_images/female/female_0.jpg\")\n",
        "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "X.append(gray.flatten())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460c16a3-cdee-43cf-a203-8b413b6ee1b0",
      "metadata": {
        "id": "460c16a3-cdee-43cf-a203-8b413b6ee1b0",
        "outputId": "e7feb2a8-32ba-4bfc-e8d2-6f3c0bdd654e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "female_0.jpg\n",
            "female_1.jpg\n",
            "female_10.jpg\n",
            "female_100.jpg\n",
            "female_101.jpg\n",
            "female_102.jpg\n",
            "female_103.jpg\n",
            "female_104.jpg\n",
            "female_105.jpg\n",
            "female_106.jpg\n",
            "female_107.jpg\n",
            "female_108.jpg\n",
            "female_109.jpg\n",
            "female_11.jpg\n",
            "female_110.jpg\n",
            "female_111.jpg\n",
            "female_112.jpg\n",
            "female_113.jpg\n",
            "female_114.jpg\n",
            "female_115.jpg\n",
            "female_116.jpg\n",
            "female_117.jpg\n",
            "female_118.jpg\n",
            "female_119.jpg\n",
            "female_12.jpg\n",
            "female_120.jpg\n",
            "female_121.jpg\n",
            "female_122.jpg\n",
            "female_123.jpg\n",
            "female_124.jpg\n",
            "female_125.jpg\n",
            "female_126.jpg\n",
            "female_127.jpg\n",
            "female_128.jpg\n",
            "female_129.jpg\n",
            "female_13.jpg\n",
            "female_130.jpg\n",
            "female_131.jpg\n",
            "female_132.jpg\n",
            "female_133.jpg\n",
            "female_134.jpg\n",
            "female_135.jpg\n",
            "female_136.jpg\n",
            "female_137.jpg\n",
            "female_138.jpg\n",
            "female_139.jpg\n",
            "female_14.jpg\n",
            "female_140.jpg\n",
            "female_141.jpg\n",
            "female_142.jpg\n",
            "female_143.jpg\n",
            "female_144.jpg\n",
            "female_145.jpg\n",
            "female_146.jpg\n",
            "female_147.jpg\n",
            "female_148.jpg\n",
            "female_149.jpg\n",
            "female_15.jpg\n",
            "female_16.jpg\n",
            "female_17.jpg\n",
            "female_18.jpg\n",
            "female_19.jpg\n",
            "female_2.jpg\n",
            "female_20.jpg\n",
            "female_21.jpg\n",
            "female_22.jpg\n",
            "female_23.jpg\n",
            "female_24.jpg\n",
            "female_25.jpg\n",
            "female_26.jpg\n",
            "female_27.jpg\n",
            "female_28.jpg\n",
            "female_29.jpg\n",
            "female_3.jpg\n",
            "female_30.jpg\n",
            "female_31.jpg\n",
            "female_32.jpg\n",
            "female_33.jpg\n",
            "female_34.jpg\n",
            "female_35.jpg\n",
            "female_36.jpg\n",
            "female_37.jpg\n",
            "female_38.jpg\n",
            "female_39.jpg\n",
            "female_4.jpg\n",
            "female_40.jpg\n",
            "female_41.jpg\n",
            "female_42.jpg\n",
            "female_43.jpg\n",
            "female_44.jpg\n",
            "female_45.jpg\n",
            "female_46.jpg\n",
            "female_47.jpg\n",
            "female_48.jpg\n",
            "female_49.jpg\n",
            "female_5.jpg\n",
            "female_50.jpg\n",
            "female_51.jpg\n",
            "female_52.jpg\n",
            "female_53.jpg\n",
            "female_54.jpg\n",
            "female_55.jpg\n",
            "female_56.jpg\n",
            "female_57.jpg\n",
            "female_58.jpg\n",
            "female_59.jpg\n",
            "female_6.jpg\n",
            "female_60.jpg\n",
            "female_61.jpg\n",
            "female_62.jpg\n",
            "female_63.jpg\n",
            "female_64.jpg\n",
            "female_65.jpg\n",
            "female_66.jpg\n",
            "female_67.jpg\n",
            "female_68.jpg\n",
            "female_69.jpg\n",
            "female_7.jpg\n",
            "female_70.jpg\n",
            "female_71.jpg\n",
            "female_72.jpg\n",
            "female_73.jpg\n",
            "female_74.jpg\n",
            "female_75.jpg\n",
            "female_76.jpg\n",
            "female_77.jpg\n",
            "female_78.jpg\n",
            "female_79.jpg\n",
            "female_8.jpg\n",
            "female_80.jpg\n",
            "female_81.jpg\n",
            "female_82.jpg\n",
            "female_83.jpg\n",
            "female_84.jpg\n",
            "female_85.jpg\n",
            "female_86.jpg\n",
            "female_87.jpg\n",
            "female_88.jpg\n",
            "female_89.jpg\n",
            "female_9.jpg\n",
            "female_90.jpg\n",
            "female_91.jpg\n",
            "female_92.jpg\n",
            "female_93.jpg\n",
            "female_94.jpg\n",
            "female_95.jpg\n",
            "female_96.jpg\n",
            "female_97.jpg\n",
            "female_98.jpg\n",
            "female_99.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "female_imgs=os.listdir(\"gender_images/female/\")\n",
        "for img in female_imgs:\n",
        "    print(img)\n",
        "    #cv2.imread(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff073f6-c5a6-41d5-b81a-8bda6405b49e",
      "metadata": {
        "id": "6ff073f6-c5a6-41d5-b81a-8bda6405b49e"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "X=[]\n",
        "y=[]\n",
        "female_imgs=glob.glob(\"gender_images/female/*.*\")\n",
        "for img in female_imgs:\n",
        "    img_3d=cv2.imread(img)\n",
        "    gray_2d=cv2.cvtColor(img_3d,cv2.COLOR_BGR2GRAY)\n",
        "    gray_2d=cv2.resize(gray_2d,(90,90))\n",
        "    X.append(gray_2d.flatten())\n",
        "    y.append('female')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a137ec42-c739-47c1-a68c-3a816f862d17",
      "metadata": {
        "id": "a137ec42-c739-47c1-a68c-3a816f862d17"
      },
      "outputs": [],
      "source": [
        "male_imgs=glob.glob(\"gender_images/male/*.*\")\n",
        "for img in male_imgs:\n",
        "    img_3d=cv2.imread(img)\n",
        "    gray_2d=cv2.cvtColor(img_3d,cv2.COLOR_BGR2GRAY)\n",
        "    gray_2d=cv2.resize(gray_2d,(90,90))\n",
        "    X.append(gray_2d.flatten())\n",
        "    y.append('male')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce6e9ab-7c21-40ed-83c9-587b5a8f7d3c",
      "metadata": {
        "id": "5ce6e9ab-7c21-40ed-83c9-587b5a8f7d3c"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c86600-0d6d-4d47-a496-f53e40a88833",
      "metadata": {
        "id": "88c86600-0d6d-4d47-a496-f53e40a88833"
      },
      "outputs": [],
      "source": [
        "X_new=np.array(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba17eb0-16fc-4e51-aa26-a77ad3ad154d",
      "metadata": {
        "id": "bba17eb0-16fc-4e51-aa26-a77ad3ad154d",
        "outputId": "b1244395-ae49-4687-9a1c-ddd4f7eccc24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300, 8100)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b00962-1c59-4b1c-a0d0-e74cb46f4d01",
      "metadata": {
        "id": "09b00962-1c59-4b1c-a0d0-e74cb46f4d01"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "face_eye_detection_in_video_&_Gender_prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}